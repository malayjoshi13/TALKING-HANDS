{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1SqdXvOICIGs1bMmjWLrCMXw62hjEtjqo","timestamp":1621180761673},{"file_id":"1eFIf0YLuAaO9BhLbrIe3Cke9JEL6ppME","timestamp":1621170990325}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"RFjFR1vIvX4d"},"source":["**STEP 1.1) MOUNTING GOOGLE DRIVE**\n"]},{"cell_type":"code","metadata":{"id":"s_cuMYIhH0zo"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LgPS7BFPcffu"},"source":["**STEP 1.2) IMPORTING LIBRARIES**"]},{"cell_type":"code","metadata":{"id":"FzA6mb78FgLr"},"source":["import keras\n","from keras.layers import Input, Dense, Flatten, Dropout, GlobalAveragePooling2D\n","from keras.models import Model\n","from keras.applications import VGG16\n","from keras.preprocessing.image import ImageDataGenerator\n","import numpy as np\n","from keras.callbacks import ModelCheckpoint\n","from glob import glob\n","from keras.optimizers import Adam\n","import matplotlib.pyplot as plt\n","from keras.utils import plot_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hls_VDF8xBwb"},"source":["**STEP 1.3) SCRIPTING THE FLOW OF TRAINING AND VALIDATION DATASET IN BATCHED FORMAT**"]},{"cell_type":"code","metadata":{"id":"ubzJxZ0eTkYG"},"source":["# Here we apply data augmentation on training and testing dataset by distorting them a little bit. This improve accuracy as model is able to see different variations thus better ability to generalise\n","# what it has memorized rather than just mugging up.\n","training_datagen = ImageDataGenerator(rescale = 1./255,\n","                                   shear_range = 0.2,\n","                                   zoom_range = 0.2,\n","                                   horizontal_flip = True)\n","\n","validation_datagen = ImageDataGenerator(rescale = 1./255)\n","\n","# Next we initiaite data generators which will divide whole training and validation datasets in smaller batches of 8 images in each. This is helpful as it avoids all images to get loaded on RAM at once.\n","batch_size = 8\n","\n","training_batched_dataset = training_datagen.flow_from_directory('/content/drive/My Drive/TalkingHand/final_dataset/train',\n","                                                 target_size = (64, 64),\n","                                                 batch_size = batch_size,\n","                                                 class_mode = 'categorical',\n","                                                 shuffle = True)\n","\n","validation_batched_dataset = validation_datagen.flow_from_directory('/content/drive/My Drive/TalkingHand/final_dataset/validation',\n","                                            target_size = (64, 64),\n","                                            batch_size = batch_size,\n","                                            class_mode = 'categorical',\n","                                            shuffle = True)\n","\n","label_mapping = (generator.class_indices)\n","print(label_mapping)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KD0R1h2zw0Je"},"source":["**STEP 1.4) CREATING ARCHITECTURE OF MODEL TO BE TRAINED**"]},{"cell_type":"code","metadata":{"id":"qr0MmNtLS5Xw"},"source":["# Now we call VGG16 network. Then we set size of images which input layer of VGG16 accepts as the size of our training images. In this way prepare VGG16 to accept our training images as input.\n","# Next we set weight of VGG16 model as weight which was obtained by training this network on Imagenet dataset. We did this to avoid situation where weights would have initialised by some random value and then \n","# it would take long time for model to fit itself on our custom made dataset. Imagenet weights are good base to start training the network.\n","# WE also set “include_top” argument to False. By doing this the fully-connected output layers of the model which are used to make predictions are not loaded, thus allowing a new output layer \n","# to be added and trained on our custom made dataset.\n","\n","# To study more about tranfer learning and fine tuning please refer: https://machinelearningmastery.com/how-to-use-transfer-learning-when-developing-convolutional-neural-network-models/\n","base = VGG16(input_shape=[64, 64] + [3], weights='imagenet', include_top=False)\n","\n","# Now we fine-tune VGG16 architecture by training the all the layers of loaded VGG16 layers. We didn't went with just using memory stored in these layers rather we ask them to train again because \n","# our custom model is very different from Imagenet dataset, thus asking layers of loaded VGG16 will not to anything good during prediction time.\n","for layer in base.layers:\n","  layer.trainable = True\n"," \n","# It is used to get number of classes/labels in our dataset.\n","folders = glob('/content/drive/My Drive/TalkingHand/final_dataset/train/*')\n","\n","# We added more layers above the loaded VGG16 network to increase capacity of overall network to understand and remember features of training images. \n","x = base.output\n","x = GlobalAveragePooling2D()(x)\n","x=Dense(1024,activation='relu')(x) \n","x = Dropout(0.5)(x) \n","x=Dense(512,activation='relu')(x)\n","prediction = Dense(len(folders), activation='softmax')(x)\n","\n","# Then we create a model object, i.e. define the entrance and exit points of the complete model which is about to be trained. This complete model includes VGG16 + additional layers added on top of VGG16\n","model = Model(inputs=base.input, outputs=prediction)\n","\n","# View the structure of the model.\n","# Firstly by means of:\n","model.summary()\n","# & secondly by means of:\n","plot_model(model, to_file='/content/drive/My Drive/TalkingHand/model_structure.png', show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C0M51XTxxS62"},"source":["**STEP 1.5) COMPILING THE MODEL**"]},{"cell_type":"code","metadata":{"id":"VMOdFUI-FbKM"},"source":["# Then we compile the model, i.e. set the parameters that keep in check how training process and backpropagation is going on\n","model.compile(\n","  loss='categorical_crossentropy',\n","  optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n","  metrics=['accuracy']\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3YejHdAXxXZG"},"source":["**STEP 1.6) STARTING TRAINING PROCESS**"]},{"cell_type":"code","metadata":{"id":"D8kW4h01pzcL"},"source":["# Now we kick start training process by setting few parameters like:\n","# \"training_batched_dataset\":- this is the path of batched training dataset coming straight from the step 1.3.\n","# \"validation_data\":- this is the path of batched validation dataset coming straight from the step 1.3.\n","# \"epochs\":- these are number of iterations/rounds for which our model will observe the whole training data. After each epoch, model on basis of its \n","#            observations made during that epoch, will update its weights by doing backpropagation \n","# \"steps_per_epoch\":- so these are number of steps or you say parts in which during each epoch model will see the whole dataset. So let us assume if total length of training dataset is 100 and we decide\n","#                     that batch_size will be \"5\", then for each epoch, model will see the whole dataset of 100 images, in 20 parts/steps (=100/5 = steps_per_epoch = (training_batched_dataset.samples)//batch_size) \n","#                     such that in each of the 20 times/steps it will see only 5 (= batch size) images out of total 100 images.\n","# \"validation_steps\":- same as above but for validation phase\n","# \"use_multiprocessing\" and \"workers\":- it will create batches of dataset by using CPU while in parallel GPU will do training over those batches. \n","#                                       Thus it will enable both CPU and GPU to keep working parallely.\n","# \"callbacks\":- using this parameter we utilised \"ModelCheckpoint\" functionality of Keras which enables to keep saving the weights (you can also save whole model)after each epoch based on certain \n","#               conditions. Like we used condition that save the whole model+weights+the compiled parameters after each epoch if and if \"val_accuracy\" has improvised comparitive to \n","#               validation accuracy of previous epoch.\n","\n","# After setting all these parameters we start the training process....wohooooooo!!\n","r = model.fit(\n","  training_batched_dataset,\n","  validation_data=validation_batched_dataset,\n","  epochs=20,\n","  steps_per_epoch=(training_batched_dataset.samples)//batch_size,\n","  validation_steps=(validation_batched_dataset.samples)//batch_size,\n","  use_multiprocessing=True,\n","  workers=1,  \n","  callbacks=[ModelCheckpoint(\"/content/drive/My Drive/TalkingHand/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5\", save_weights_only=False, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jzGBsd8yxdQv"},"source":["**STEP 1.7) PLOTTING LOSS AND ACCURACIES CURVES**"]},{"cell_type":"code","metadata":{"id":"UISdYrCcU9oL"},"source":["# Then we plot loss curves\n","plt.plot(r.history['loss'], label='train loss')\n","plt.plot(r.history['val_loss'], label='val loss')\n","plt.legend()\n","plt.show()\n","plt.savefig('LossVal_loss')\n","\n","# and accuracies curves\n","plt.plot(r.history['accuracy'], label='train acc')\n","plt.plot(r.history['val_accuracy'], label='val acc')\n","plt.legend()\n","plt.show()\n","plt.savefig('AccVal_acc')"],"execution_count":null,"outputs":[]}]}